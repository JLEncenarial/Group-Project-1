{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I plan to start this off. (Jean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just took whatever libraries he put on from class notes so I don't go back and forth. Will change later (maybe?)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "train_df = pd.read_csv(\"train.csv.zip\")\n",
    "train_df.head()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (2000,)\n",
      "vectorized: (2000, 182311)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 02</th>\n",
       "      <th>00 02 august</th>\n",
       "      <th>00 02 august 11</th>\n",
       "      <th>00 2007</th>\n",
       "      <th>00 44</th>\n",
       "      <th>00 bst</th>\n",
       "      <th>00 bst understand</th>\n",
       "      <th>00 bst understand evening</th>\n",
       "      <th>000</th>\n",
       "      <th>...</th>\n",
       "      <th>翻译</th>\n",
       "      <th>翻译 თარგმანი</th>\n",
       "      <th>翻译 თარგმანი μεταφραση</th>\n",
       "      <th>翻译 თარგმანი μεταφραση અન</th>\n",
       "      <th>見学</th>\n",
       "      <th>見学 迷惑</th>\n",
       "      <th>迷惑</th>\n",
       "      <th>連絡</th>\n",
       "      <th>連絡 見学</th>\n",
       "      <th>連絡 見学 迷惑</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 02  00 02 august  00 02 august 11  00 2007  00 44  00 bst  \\\n",
       "1993  0.0    0.0           0.0              0.0      0.0    0.0     0.0   \n",
       "1869  0.0    0.0           0.0              0.0      0.0    0.0     0.0   \n",
       "1359  0.0    0.0           0.0              0.0      0.0    0.0     0.0   \n",
       "51    0.0    0.0           0.0              0.0      0.0    0.0     0.0   \n",
       "1747  0.0    0.0           0.0              0.0      0.0    0.0     0.0   \n",
       "\n",
       "      00 bst understand  00 bst understand evening  000  ...   翻译  \\\n",
       "1993                0.0                        0.0  0.0  ...  0.0   \n",
       "1869                0.0                        0.0  0.0  ...  0.0   \n",
       "1359                0.0                        0.0  0.0  ...  0.0   \n",
       "51                  0.0                        0.0  0.0  ...  0.0   \n",
       "1747                0.0                        0.0  0.0  ...  0.0   \n",
       "\n",
       "      翻译 თარგმანი  翻译 თარგმანი μεταφραση  翻译 თარგმანი μεταφραση અન   見学  \\\n",
       "1993          0.0                    0.0                       0.0  0.0   \n",
       "1869          0.0                    0.0                       0.0  0.0   \n",
       "1359          0.0                    0.0                       0.0  0.0   \n",
       "51            0.0                    0.0                       0.0  0.0   \n",
       "1747          0.0                    0.0                       0.0  0.0   \n",
       "\n",
       "      見学 迷惑   迷惑   連絡  連絡 見学  連絡 見学 迷惑  \n",
       "1993    0.0  0.0  0.0    0.0       0.0  \n",
       "1869    0.0  0.0  0.0    0.0       0.0  \n",
       "1359    0.0  0.0  0.0    0.0       0.0  \n",
       "51      0.0  0.0  0.0    0.0       0.0  \n",
       "1747    0.0  0.0  0.0    0.0       0.0  \n",
       "\n",
       "[5 rows x 182311 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing Data and Processessing\n",
    "text = train_df['comment_text'].head(n=2000)\n",
    "y = train_df['insult'].head(n=2000)\n",
    "\n",
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(text)\n",
    "\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(text)\n",
    "tok_cols = vec_tf.get_feature_names_out()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", text.shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,15):\n",
    "\t\tn = i*10\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LinearSVC(max_iter=100, tol=.01))]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df[0:1000], y[0:1000])\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_df['comment_text'].head(n=2000)\n",
    "y = train_df['insult'].head(n=2000)\n",
    "\n",
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(text)\n",
    "\n",
    "tok_cols2 = tf_idf.get_feature_names_out()\n",
    "tmp_df = pd.DataFrame(tmp_vec.toarray(), columns=tok_cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 125773)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tmp_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our Baselien accuarcy\n",
    "text = train_df['comment_text'].head(n=2000)\n",
    "y = train_df['insult'].head(n=2000)\n",
    "\n",
    "tf_idf_base = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\", max_features=2000)\n",
    "tmp_vec_base = tf_idf_base.fit_transform(text)\n",
    "\n",
    "tok_cols2_base = tf_idf_base.get_feature_names_out()\n",
    "tmp_df_base = pd.DataFrame(tmp_vec_base.toarray(), columns=tok_cols2_base)\n",
    "X_tr_base, X_te_base, y_tr_base, y_te_base = train_test_split(tmp_df_base, y)\n",
    "\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr_base, y_tr_base)\n",
    "pipe_test.score(X_te_base, y_te_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Accuarcy: 0.952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tmp = TruncatedSVD(n_components=10)\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"svd\", svd_tmp), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr, y_tr)\n",
    "pipe_test.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['125', '1930', 'cambridge', 'cambridge university', 'central european', 'dem']\n",
      "Topic:1  ['christianity', 'southgate', 'myths', 'renaissance', 'reviews', '100 southgate']\n",
      "Topic:2  ['kacic', 'dinaric', 'dalmatian', 'famous', '1547', '1547 contemporary']\n",
      "Topic:3  ['14 november', 'did feel', 'discussion right', 'en wiki', 'experiences', 'formatting style']\n",
      "Topic:4  ['gamecube', 'spaces', 'mode', 'minigame', 'minigames', 'pot']\n",
      "Topic:5  ['anarcho', 'moral philosophy', 'philosophical conversation', 'philosophical questions', 'philosophy particularly', 'stefan']\n",
      "Topic:6  ['article clearly', 'cycle', 'escalating', 'opportunity suggest', 'trading', 'alexander great']\n",
      "Topic:7  ['reserved', 'reserved image', 'rights reserved', 'rights reserved image', 'non commercial', 'creative commons']\n",
      "Topic:8  ['edits just', 'need make', 'procedures', 'soapbox', 'symbols', 'use real']\n",
      "Topic:9  ['randroide', 'agree course', 'article sources', 'does make sense', 'enable', 'jokes']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "nltk.download('all')\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(tok)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stemTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.stemmer.stem(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.lemmatizer.lemmatize(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     16\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__max_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m1500\u001b[39m],\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n\u001b[0;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__norm\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m             }\n\u001b[0;32m     21\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator  \u001b[38;5;241m=\u001b[39m pipe2, \n\u001b[0;32m     22\u001b[0m                                param_grid \u001b[38;5;241m=\u001b[39m params, \n\u001b[0;32m     23\u001b[0m                                scoring    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     26\u001b[0m best \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     27\u001b[0m preds \u001b[38;5;241m=\u001b[39m best\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:754\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    751\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    753\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m--> 754\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    755\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:813\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    811\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:266\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     _kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score(partial(_cached_call, \u001b[38;5;28;01mNone\u001b[39;00m), estimator, X, y_true, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:353\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_overlap(\n\u001b[0;32m    346\u001b[0m     message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is an overlap between set kwargs of this scorer instance and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m    352\u001b[0m )\n\u001b[1;32m--> 353\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m method_caller(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[0;32m    354\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:86\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[1;32m---> 86\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m _get_response_values(\n\u001b[0;32m     87\u001b[0m     estimator, \u001b[38;5;241m*\u001b[39margs, response_method\u001b[38;5;241m=\u001b[39mresponse_method, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_response.py:85\u001b[0m, in \u001b[0;36m_get_response_values\u001b[1;34m(estimator, X, response_method, pos_label)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     83\u001b[0m     pos_label \u001b[38;5;241m=\u001b[39m pos_label \u001b[38;5;28;01mif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m classes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 85\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m prediction_method(X)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:507\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    505\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 507\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2150\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \n\u001b[0;32m   2135\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2148\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2150\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1428\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1428\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1430\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mstemTokenizer.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words:\n\u001b[1;32m---> 11\u001b[0m         filtered_tok\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer\u001b[38;5;241m.\u001b[39mstem(tok))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_tok\n",
      "File \u001b[1;32mc:\\Users\\Jean\\anaconda3\\Lib\\site-packages\\nltk\\stem\\snowball.py:1704\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# STEP 3\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step3_suffixes:\n\u001b[1;32m-> 1704\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[0;32m   1705\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r1\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[0;32m   1706\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtional\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "vec_tf = TfidfVectorizer()\n",
    "model_svc = SVC()\n",
    "\n",
    "y = train_df[\"insult\"].head(n=7000)\n",
    "X = train_df[\"comment_text\"].head(n=7000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe2 = Pipeline([ \n",
    "                    #(\"vect\", vec_cv),\n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = {\"vect__max_features\":[100,500,1000,1500],\n",
    "            \"vect__tokenizer\":(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n",
    "            \"vect__norm\":[\"l1\",\"l2\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(estimator  = pipe2, \n",
    "                               param_grid = params, \n",
    "                               scoring    = \"balanced_accuracy\")\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "best = grid.best_estimator_\n",
    "preds = best.predict(X_test)\n",
    "print(best)\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
